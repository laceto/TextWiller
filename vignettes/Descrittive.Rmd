---
title: "Statistiche Descrittive"
author: "Mattia Da Pont"
date: "29/10/2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistiche Descrittive}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

In questa vignette vengono presentati esempi di statistiche descrittive che è stato possibile creare dopo aver preprocessato i dati tramite TextWiller.
La funzione normalizzaTesti, richiamando le numerose funzioni di pulizia del testo all'interno di TextWiller hanno preparato i dati ad essere poi descritti e visualizzati al netto di punteggiatura, congiunzioni e più in generale, caratteri o parole che non apporterebbero informazione.
I codici di esempio sono stati presi da "Text Mining with R" (Silge and Robinson, 2017).
Sono stati usati i pacchetti R tidytext, tidyr, dplyr, ggplot, ggraph e ggpubr.


```{r,message=FALSE,echo=FALSE,warning=FALSE}

library(TextWiller)
library(tm)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(scales)
library(igraph)
library(ggraph)
library(widyr)
library(ggpubr)


data("quotidiani_2_ottobre")
data<-tw
rm(tw)

for(i in 1:nrow(data)){
  if(data$screen_name[i]=="repubblica") data$screen_name[i]<-"laRepubblica"
  else if (data$screen_name[i]=="Corriere") data$screen_name[i]<-"ilCorriere"
}
data$text<-normalizzaTesti(data$text,normalizzaEmoticons = FALSE)



```


```{r}

text<-normalizzaTesti(data$text)
tweet.text<-tibble(text,quotidiano=data$screen_name,tweet=1:9674)
tweet.text$quotidiano<-as.factor(tweet.text$quotidiano)


```


```{r}


tweet.text<-tibble(text,quotidiano=data$screen_name,tweet=1:9674)
tweet.text<-na.omit(tweet.text)

tweet.text$quotidiano<-as.factor(tweet.text$quotidiano)

```




Frequenze assolute delle parole presenti nei tweet

```{r}

tidy.tweets <- tweet.text %>%
  unnest_tokens(word, text)


#frequenze totali


tidy.tweets %>%                       
  count(as.character(word), sort = TRUE)



tidy.tweets %>%
  count(word, sort = TRUE) %>%
  filter(n > 300) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


###############
#frequenze divise x quotidiano

#la Repubblica
tidy.repubblica<-tweet.text[tweet.text$quotidiano=="laRepubblica",] %>%
  unnest_tokens(word, text)

#il Corriere
tidy.corriere<-tweet.text[tweet.text$quotidiano=="ilCorriere",] %>%
  unnest_tokens(word, text)

#La Stampa
tidy.lastampa<-tweet.text[tweet.text$quotidiano=="LaStampa",] %>%
  unnest_tokens(word, text)




tidy.repubblica %>%
  count(as.character(word), sort = TRUE)


tidy.corriere %>%                       
  count(as.character(word), sort = TRUE)

tidy.lastampa %>%                       
  count(as.character(word), sort = TRUE)



g1<-tidy.repubblica %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="cornflowerblue") +
  xlab("laRepubblica") +
  coord_flip()


g2<-tidy.corriere %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="brown1") +
  xlab("ilCorriere") +
  coord_flip()


g3<-tidy.lastampa %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="green4") +
  xlab("La Stampa") +
  coord_flip()

ggarrange(g1, g2, g3,
          ncol = 2, nrow = 2)


```


Analizziamo quindi la frequenza delle parole prendendo a coppie i quotidiani. Le parole che si trovano vicino alla bisettrice hanno frequenze simili.

```{r,warning=FALSE}


frequencyRep <- bind_rows(mutate(tidy.repubblica, quotidiano = "repubblica"),
                          mutate(tidy.corriere, quotidiano = "Corriere"), 
                          mutate(tidy.lastampa, quotidiano = "LaStampa")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(quotidiano, word) %>%
  group_by(quotidiano) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(quotidiano, proportion) %>% 
  gather(quotidiano, proportion,`Corriere`:`LaStampa`)


frequencyLaS <- bind_rows(mutate(tidy.lastampa, quotidiano = "LaStampa"),
                          mutate(tidy.corriere, quotidiano = "Corriere")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(quotidiano, word) %>%
  group_by(quotidiano) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(quotidiano, proportion) %>% 
  gather(quotidiano, proportion,`Corriere`)


g1<-ggplot(frequencyRep, aes(x = proportion, y = `repubblica`, color = abs(`repubblica` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~quotidiano, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Repubblica", x = NULL)



g2<-ggplot(frequencyLaS, aes(x = proportion, y = `LaStampa`, color = abs(`LaStampa` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~quotidiano, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "La Stampa", x = NULL)


ggarrange(g1, g2,
          ncol = 1, nrow = 2)



```

Controlliamo la TF-IDF




```{r}

quotidiano_words <- tweet.text %>%
  unnest_tokens(word, text) %>%
  filter(word!="urlw")%>%
  count(quotidiano, word, sort = TRUE)

total_words <- quotidiano_words %>% 
  group_by(quotidiano) %>% 
  summarize(total = sum(n))

quotidiano_words <- left_join(quotidiano_words, total_words)


quotidiano_words <- quotidiano_words %>%
  bind_tf_idf(word, quotidiano, n)


quotidiano_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(quotidiano) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = quotidiano)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  facet_wrap(~quotidiano, ncol = 2, scales = "free") +
  coord_flip()


```

Visualizziamo i bigrammi.

```{r}

tweet_bigrams <- tweet.text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


bigrams_separated <- tweet_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")


bigram_counts <- bigrams_separated %>% 
  count(word1, word2, sort = TRUE)



#la repubblica



bigrams_separated_rep <- tweet_bigrams %>%
  filter(quotidiano=="laRepubblica")%>%
  count(bigram, sort = TRUE)

bigrams_separated_rep

#il corriere

bigrams_separated_corr <- tweet_bigrams %>%
  filter(quotidiano=="ilCorriere")%>%
  count(bigram, sort = TRUE)

bigrams_separated_corr

#La Stampa
  

bigrams_separated_las <- tweet_bigrams %>%
  filter(quotidiano=="LaStampa")%>%
  count(bigram, sort = TRUE)

bigrams_separated_las



```

Tramite i bigrammi costruiamo delle reti formate da catene di questi ultimi.
```{r, message=FALSE, echo=FALSE}






tweet.text<-tibble(text,quotidiano=data$screen_name,tweet=1:9674)
tweet.text$quotidiano<-as.factor(tweet.text$quotidiano)


tweet_bigrams <- tweet.text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
  



```


```{r}


#la Repubblica 


bigrams_separated <- tweet_bigrams %>%
  filter(quotidiano=="laRepubblica")%>%
  separate(bigram, c("word1", "word2"), sep = " ")


bigram_counts <- bigrams_separated %>% 
  filter(quotidiano=="laRepubblica")%>%
  count(word1, word2, sort = TRUE)


bigram_graph <- bigram_counts %>%
  filter(n > 23) %>%
  graph_from_data_frame()

set.seed(123)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()



#il Corriere 



bigrams_separated <- tweet_bigrams %>%
  filter(quotidiano=="ilCorriere")%>%
  separate(bigram, c("word1", "word2"), sep = " ")


bigram_counts <- bigrams_separated %>% 
  filter(quotidiano=="ilCorriere")%>%
  count(word1, word2, sort = TRUE)


bigram_graph <- bigram_counts %>%
  filter(n > 7) %>%
  graph_from_data_frame()

set.seed(123)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()



# La Stampa


bigrams_separated <- tweet_bigrams %>%
  filter(quotidiano=="LaStampa")%>%
  separate(bigram, c("word1", "word2"), sep = " ")


bigram_counts <- bigrams_separated %>% 
  filter(quotidiano=="LaStampa")%>%
  count(word1, word2, sort = TRUE)


bigram_graph <- bigram_counts %>%
  filter(n > 6) %>%
  graph_from_data_frame()

set.seed(123)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()




```











